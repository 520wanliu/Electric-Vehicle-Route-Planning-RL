### 基于强化学习的谷歌地图电动汽车路线规划

更多详情请查看==论文==的实验部分。本工作旨在考虑电池、电机、行驶时间等因素，在一定条件下为车辆生成更好的路线(使能源消耗最小化)。

在您开始运行程序之前，请仔细阅读此页。有许多参数或条件需要修改，以适应您的环境。

#### Python 3.6.2的库要求:

(1) Tensorflow (CPU) 

(2) numpy

 (3) pandas 

(4) haversine

 (5) time

 (6) random 

(7) math

 (8) requests

 (9) urllib

#### 设置谷歌地图API

谷歌地图 API要求每个搜索URL都包含用户的键。请参考这个链接:https://cloud.google.com/maps-platform/。输入你的账单信息，它现在不会向你收费，因为你获得了200美元的免费搜索积分。如果你超过了一定的搜索量，你将会被收费。您只需要Maps选项。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220708104853291.png" alt="image-20220708104853291"  />

您将获得一个密钥，例如AZuesYuds12_dsakd23456sdeHf。那么搜索URL(纬度，经度)=(40.468254，-86.980963)将变成:https://maps.googleapis.com/maps/api/geocode/json?address=40.468254%2C-86.980963&key=AZuesYuds12_dsakd23456sdeHf

URL的结果(带有正确的键)将如下图所示。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220708105010384.png" alt="image-20220708105010384"  />

您可以在**Environment.py**中学习如何正确地与谷歌地图api(地理代码/海拔/方向)交互。

#### 如何开始？

下载所有python文件如下:main.py, Environment.py, DoubleDQN.py, battery.py, motor.py

- 在main.py中输入你的起始位置:位置名称或地理代码(lat, lng)
- 在main.py中输入你的目的地位置:位置名称或地理代码(lat, lng)
- 在main.py第78行中输入每一步的长度(step_length)，更高但不准确(例如:1000米比100米训练时间更少)
- 在main.py输入你想训练多少集
- main.py中有许多文件和文件夹的路径名称需要更改以适应您的条件
- 确保您在培训过程中能够访问internet和谷歌地图。请注意，如果您对谷歌地图API的访问在24小时内超过限制，服务器将被阻塞，程序将卡住。在main.py中，我们实现了一种机制，可以在查询数据时让程序休眠一段时间。如果您能够获得谷歌映射API的完全访问权，那么您就可以删除sleep命令并使学习过程更快。该规则可能会随着谷歌地图Server采用的新规则而改变。如果是，请禁用所有睡眠命令~编辑在2019年4月)
- 如果您还没有训练过任何模型，那么您需要在main.py的第141行中使**load_model == False**
- 运行文件main.py

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220708105528230.png" alt="image-20220708105528230" style="zoom: 50%;" />

#### 如何创建？

- 你可以模拟一个真实的电池系统，包括电池退化、SOC和其他因素，使整个系统更像一辆真正的汽车。该模型可以在python文件中实现：battery.py。在原始的python文件中，我们只对电池进行线性建模。
- 你可以模拟一个真实的电机系统，它可以包括电机的疲劳，热状态和其他因素来模拟真实的电机。你的模型可以在python文件中实现：motor.py。在最初的python文件中，我们只以理想的方式建模电机。你可以找出在电机输入能量和输出能量之间的真实因素，并在代码中应用它。
- 您可以使用更复杂的神经网络体系结构来处理更大的映射边界，并在文件DoubleDQN.py中实现。这个python文件的功能是从环境中获取输入(比如状态)并输出一个动作(可以是Q值)
- 您可以在main.py文件中实现除Double-DQN之外的其他学习算法。这个文件将创建两个.csv文件，一个记录训练环境参数名称train_para.csv，另一个名称result.csv将记录训练数据，如奖励、持续时间、失败步骤、步骤历史等。学习模型和检查点也保存在名为model的文件夹中。
- 您可以在文件的**step**功能中添加更多的动作选择：Environment.py函数中添加更多的动作选择，比如指向西南或东北。在原始的python文件中，我们只实现了4个动作:北、东、南和西。它也有可能改变你计算能源消耗的方式，并添加再生制动的概念。

#### 如何做工作?

##### Double-DQN算法

强化学习的基本思想是使agent从其采取的行动和得到的反馈中学习。当代理遇到当前状态时，它会决定下一步做什么并给出一个操作。通过执行此操作，代理将进入下一个状态。这个例程将一直持续到代理到达目的地，或者在代理应该执行的操作数量限制时终止。

我们**初始化两个相同的网络**，第一个是**q网络**，在当前状态下由代理决定动作。第二种是**target -network**，作为Q-network实现的目标。我们只<u>对Q-network中的每一步进行反向传播并使用学习率为0.0001的Adam优化器更新权值</u>，然后<u>每N步将Q-network中的权值复制到Target-network中</u>(在原始python文件中实现了5步)。

我们的学习代理是一辆电动汽车，通过选择不同的动作(北、东、南、西)在谷歌地图环境中导航。动作可以由Double-DQN决定，也可以随机决定。在学习过程中，agent首先会在地图上随机导航探索地图，但我们会逐渐减少随机选择动作的部分，而采用Double-DQN模型提供的**q值最高的动作**。我们将<u>当前状态、动作、当前奖励、下一个状态、到达终点或不到达终点保存在一个元组中，并将每个元组存储在回放缓冲区中</u>，它就像我们存储所有内存的大脑。当我们需要更新Q-network权值时，我们将<u>从重放缓冲区中随机均匀地选取Nb个数的元组</u>。这里是Target-network的动作，它将元组的下一个状态作为输入，并输出对应于每个动作选择的q值向量。我们只会在这个q值向量中选取一个值，这个值对应于在将下一个状态输入q网络时给予我们最高值的行动，然后我们将这个值与γ因子添加到当前奖励中，从而计算出目标y。在原始情况下，我们实现了γ为0.9,Nb为32。

给定回放缓冲区中的一个元组，损失是通过减去q值来计算的，q值是通过将当前状态输入到q网络中并从y中选择对应动作的值来计算的。然后我们将所有损失加起来(在我们的例子中有32个损失)并使用它来更新q网络的权值。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220708111357841.png" alt="image-20220708111357841"  />

谷歌深度学习(链接：https://deepmind.com/research/publications/deep-reinforcement-learning-double-q-learning/)

##### 学习环境

我们将地图做成网格地图，供学习代理在上面导航，如图(a)所示。严格地说，网格地图中的每个网格不是一个矩形。这一现象是由球面几何和我们对步幅长度的限制造成的，如图(b)所示。将步幅长度限制在一定米的原因是为了将误差限制在给定长度内。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220708111513587.png" alt="image-20220708111513587" style="zoom: 67%;" />

##### 与谷歌地图API互动

在动作集中有4个动作，北，东，南，西。如果您当前的位置是(纬度，经度)=(40.468254，-86.980963)，并且您想采取向北的行动，那么您的下一个位置将变成(纬度，经度)= (40.468254 + value1， -86.980963)。value1需要从step_length中计算。如果您想采取向东的行动，那么您的下一个位置将变成(纬度，经度)= (40.468254，-86.980963 + value2)。value2需要从step_length和当前纬度计算(value2将根据代理当前纬度不同，但value2的所有值应该反映step_length)。新的计算位置不能保证从当前位置访问，如果方向不可访问(例如，湖或森林)，谷歌服务器可能会返回一个状态不是“ok”。

假设agent在当前位置(A)，向南前往下一个位置(b)，显然，Directions API提供的路线是395公路，距离大于1000m。我们可以得到导航指令列表，其形式为:{A的地理编码，A到1的持续时间，A到1的距离，地理编码1}，{地理编码1，持续时间1到2，距离1到2，地理编码2}....其中A, 1, 2,3, B在Directions API中输入A和B的地理编码后如图6所示。指令的数量基于方向API，在下图中有四条指令，从A到b。我们使用导航指令列表中的每个地理代码从海拔API获得每个位置的高度，并计算每个指令中的高度。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709085003290.png" alt="image-20220709085003290" style="zoom:67%;" />

##### 能量计算

我们通过下面的流程图计算能量，这个符号对应于前面的图。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709085232844.png" alt="image-20220709085232844" style="zoom:67%;" />

下图显示了如何计算车辆上坡行驶的能量(也可以应用于平坦道路)。

![image-20220709085309623](C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709085309623.png)

但是请注意，我们只计算下图中显示的两个位置之间的仰角。为了提高精度，您应该最小化这两个位置之间的距离(这将增加计算时间)。

![image-20220709085535941](C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709085535941.png)

##### 电池

在实验中，电池性能不会影响训练过程。该电池能够携带50000Wh的能量，这是电动汽车制造商特斯拉的标准产品。在电化学中，我们建议使用电池90% ~ 20%的荷电状态(SOC)来提高它的寿命，我们在我们的案例中实现了这一点。SOC是通过当前能量与总能量的比率来计算的。我们不会把电池退化带入实验。进一步的工作可以将电池性能的实际因素考虑在内，作为培训过程的一部分。对于这个实验，我们只演示了在理想状态下电池需要充电多少次和消耗多少能量。

##### 奖励机制

定义奖励的基本概念是基于从当前位置到下一个位置的能量消耗，例如图6中从A到B。能量是用前一节提供的方法计算的。然后把能量除以10000再乘以-1。为了使训练过程中的总步骤数最小化，如果下一个位置可达，我们在每个过渡中添加-0.1。换句话说，采取任何可到达步骤的奖励r将是r = -0.1 -(能量消耗/ 10000)。如果下一个位置是不可到达的，如湖泊或河流，r = -1，代理将停留在当前相同的位置并采取其他行动。如果下一个位置和目标位置的距离小于预先设定的位移长度(图6中的1000米)，那么采取该行动的奖励r将变成r = +1 -(从当前位置到下一个位置的能耗/ 10000)-(从下一个位置到目标位置的能耗/ 10000)。注意到这里+1出现在奖励中，因为这一行动的成功将引导代理到达目的地。

- 黑色：不可触及或超出边界
- 红色：起始位置
- 蓝色：目的地位置
- 绿色：可达位置到可达位置，不在目的地一大步的范围内
- 白色：可达位置到距离目的地一个步幅的位置

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709085914329.png" alt="image-20220709085914329" style="zoom:67%;" />

#### 结果

从起始位置(地理代码:40.4682572，-86.9803475)到目的地(地理代码:40.445283，-86.948429)，步幅为750米。在一个章节中超过64步将被视为失败。注意到在训练过程中，谷歌map api经常阻塞我们的服务器，我们被迫结束训练过程，从中断的事件中恢复模型(红线)。该问题会导致重放缓冲区为空，我们在学习过程中均匀随机选择样本来计算损失和更新权值。因此，我们将需要恢复模型，并开始随机选择动作来重新填充回放缓冲区，并逐渐减少随机动作的比例。

图9中的蓝线显示了代理消耗的能量。在600集之后，agent能够找到将能量消耗降到最低的方法。前600集的振荡是由于高度随机的作用(绿线)和Q网络提供的不准确的Q值造成的。在最小化Q值误差的同时，减小了系统的振荡，降低了系统的能量消耗，使系统更加稳定。agent随机行动所能达到的最小能量为1327(J)，对应的时间为659秒，其中谷歌地图提供的路线能量和时间为1489(J)和315秒。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709090146540.png" alt="image-20220709090146540"  />

下图为步幅为1000m时的能耗结果。在一个章节中超过36步将被视为失败。agent随机动作所能达到的最小能量为1951(J)，对应时间为946秒。我们没有让这个实验和上一个实验在同一个点结束，因为我们只是想证明一个趋势，即步幅越小，结果越好，但计算时间越长。

![image-20220709090224789](C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709090224789.png)

#### 结束语

这个作品只是我的一个概念的实验，是2017秋季ECE570人工智能的一个课程项目。参数和模型未进行优化。如果你能把更大的地图带到训练中(当然你会被收取很多谷歌的费用)，并改进模型，我相信你能得到更好的结果。

目前，输入仅仅是纬度和经度。我认为有一个严重的问题，经纬度为负值，这是我在实施这个项目时没有想到的。读者可以在此基础上进行改进。

我们没有在任何看不见的地图中验证我们的模型，这就涉及到一个基本的问题，这个模型能否在看不见的世界中使用?我不知道。

使用经度和纬度作为输入是一个好主意吗?我很怀疑，因为我做了这个项目。如果在建一些新公路，会发生什么事?我们要重新训练模型吗?也许将地图截图作为输入将是一个不错的想法，并且这一想法将与我在Paper调查部分所提到的Value Iteration Network(链接:https://arxiv.org/abs/1602.02867)类似。

#### 贡献

我从Arthur Juliani的教程网站学到了很多，它使用tensorflow实现了强化学习算法。我还引用了他的部分代码并进行了修改。(链接:https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)

我在初始化两个tensorflow网络(使用相同的模型)时也遇到了问题。下面的链接可以解决这个问题(链接:https://stackoverflow.com/questions/41577384/variable-scope-issue-in-tensorflow)。下图是我实现这个概念的地方。

<img src="C:\Users\gywan\AppData\Roaming\Typora\typora-user-images\image-20220709090427428.png" alt="image-20220709090427428" style="zoom:80%;" />













